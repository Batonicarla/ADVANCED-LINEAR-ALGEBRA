{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqkivgEHr_KW"
   },
   "source": [
    "# Formative Assignment: Advanced Linear Algebra (PCA)\n",
    "This notebook will guide you through the implementation of Principal Component Analysis (PCA). Fill in the missing code and provide the required answers in the appropriate sections. You will work with a dataset that is Africanized .\n",
    "\n",
    "Make sure to display outputs for each code cell when submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xycIp758r_Kb"
   },
   "source": [
    "### Step 1: Load and Standardize the Data\n",
    "Before applying PCA, we must standardize the dataset. Standardization ensures that all features have a mean of 0 and a standard deviation of 1, which is essential for PCA.\n",
    "Fill in the code to standardize the dataset.\n",
    "\n",
    "STRICTLY - Write code that implements standardization based on the image below\n",
    "\n",
    "<img src='data:image/jpeg;base64,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'/>\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Step 1: Load and Standardize the data (use of numpy only allowed)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/content/co2 Emission Africa.csv')\n",
    "\n",
    "print(\"Original Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xh424lTkwc7J",
    "outputId": "2ca7c4d3-befa-4100-8151-c97324d35270",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# show non-numeric columns\n",
    "non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(f\"Non-numeric columns identified: {non_numeric_cols}\")\n",
    "\n",
    "# 2. Apply One-Hot Encoding to 'Sub-Region'\n",
    "df_encoded = pd.get_dummies(df, columns=['Sub-Region'], prefix='Region')\n",
    "\n",
    "# 3. Filter for all numeric columns (now including the new Region columns)\n",
    "numeric_df = df_encoded.select_dtypes(include=[np.number])\n",
    "print(f\"Total numeric columns after encoding: {numeric_df.shape[1]}\")\n",
    "\n",
    "# 4. Handle missing values using mean imputation\n",
    "numeric_df_filled = numeric_df.fillna(numeric_df.mean())\n",
    "\n",
    "print(f\"Remaining missing values: {numeric_df_filled.isnull().sum().sum()}\")\n",
    "print(\"Cleaned data shape for PCA:\", numeric_df_filled.shape)\n",
    "numeric_df_filled.head()\n",
    "\n",
    "# remain with numeric columns only\n",
    "numeric_df =  df.select_dtypes(include=[np.number])\n",
    "print(f\"\\nNumeric columns: {numeric_df.shape[1]}\")\n",
    "print(f\"Rows: {numeric_df.shape[0]}\")\n",
    "\n",
    "# numeric missing values in dataset\n",
    "print(\"\\nMissing values in numeric data\")\n",
    "print(numeric_df.isnull().sum().sum(), \"total missing values\")\n",
    "\n",
    "# fill the missing numeric data values with column mean\n",
    "numeric_df_filled = numeric_df.fillna(numeric_df.mean())\n",
    "print(f\"\\nMissing values after filling: {numeric_df_filled.isnull().sum().sum()}\")\n",
    "\n",
    "# Show the cleaned data\n",
    "print(\"\\nCleaned numeric data shape:\", numeric_df_filled.shape)\n",
    "numeric_df_filled.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "3oQoBeS8yAZG",
    "outputId": "ad3d7423-753f-41b8-fd47-00243bf89c5c",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKihXBaBr_Kc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "outputId": "7d6f2998-d94c-4af6-c5d8-aac1edf005a8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Load and Standardize the data (use of numpy only allowed)\n",
    "\n",
    "# calculate mean and standard deviation\n",
    "data_mean = numeric_df_filled.mean()\n",
    "data_std = numeric_df_filled.std()\n",
    "\n",
    "# Do not use sklearn (Data - Data Mean)/ Data's Standard Deviation\n",
    "standardized_data = (numeric_df_filled - data_mean) / data_std\n",
    "\n",
    "\n",
    "# Display the first few rows of standardized data\n",
    "print(f\"\\nStandardized Data Shape: {standardized_data.shape}\")\n",
    "standardized_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fybn40Syr_Kd"
   },
   "source": [
    "### Step 3: Calculate the Covariance Matrix\n",
    "The covariance matrix helps us understand how the features are related to each other. It is a key component in PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbklA9tqr_Ke",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694
    },
    "outputId": "4adf06aa-2067-4ce5-946b-271442762852",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Calculate the Covariance Matrix\n",
    "cov_matrix = np.cov(standardized_data.T)\n",
    "print(f\"Covariance Matrix Shape: {cov_matrix.shape}\")\n",
    "cov_matrix\n",
    "\n",
    "# Visualization covariance matrix\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(cov_matrix, cmap='coolwarm', center=0,\n",
    "            xticklabels=numeric_df_filled.columns,\n",
    "            yticklabels=numeric_df_filled.columns,\n",
    "            cbar_kws={'label': 'Covariance'})\n",
    "plt.title('Covariance Matrix Heatmap', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWzqXsR0r_Ke"
   },
   "source": [
    "### Step 4: Perform Eigendecomposition\n",
    "Eigendecomposition of the covariance matrix will give us the eigenvalues and eigenvectors, which are essential for PCA.\n",
    "Fill in the code to compute the eigenvalues and eigenvectors of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Tm0rzdAr_Ke",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cd2a46d7-d0c6-44a3-bafb-8176cd81f9c8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Perform Eigendecomposition\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "print(f\"Number of Eigenvalues: {len(eigenvalues)}\")\n",
    "print(f\"Eigenvectors Shape: {eigenvectors.shape}\")\n",
    "print(f\"\\nEigenvalues:\\n{eigenvalues}\")\n",
    "eigenvalues, eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bp6o_4sTr_Kf"
   },
   "source": [
    "### Step 5: Sort Principal Components\n",
    "Sort the eigenvectors based on their corresponding eigenvalues in descending order. The higher the eigenvalue, the more important the eigenvector.\n",
    "Complete the code to sort the eigenvectors and print the sorted components.\n",
    "\n",
    "<a url ='https://www.youtube.com/watch?v=vaF-1xUEXsA&t=17s'>How Is Explained Variance Used In PCA?'<a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpfoRz-Xr_Kf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "229c683f-c450-48e3-c576-7ac533205961",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 5: Sort Principal Components\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 5A: DEMONSTRATE EIGENVALUE SORTING (CRITICAL FOR PCA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show UNSORTED eigenvalues first\n",
    "print(\"\\n1. UNSORTED Eigenvalues (as computed from covariance matrix):\")\n",
    "print(f\"   First 10: {eigenvalues[:10]}\")\n",
    "print(f\"   Note: These are in random order, NOT sorted by importance!\")\n",
    "\n",
    "# Now SORT in descending order\n",
    "print(\"\\n2. SORTING Eigenvalues in DESCENDING order...\")\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]  # [::-1] reverses to descending\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "print(\"   ✓ Eigenvalues sorted!\")\n",
    "print(f\"   First 10 sorted: {sorted_eigenvalues[:10]}\")\n",
    "print(f\"   Largest eigenvalue (PC1): {sorted_eigenvalues[0]:.4f}\")\n",
    "print(f\"   Smallest eigenvalue (PC{len(sorted_eigenvalues)}): {sorted_eigenvalues[-1]:.4f}\")\n",
    "\n",
    "# Calculate explained variance\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5B: CALCULATE EXPLAINED VARIANCE PERCENTAGES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_variance = np.sum(sorted_eigenvalues)\n",
    "explained_variance = sorted_eigenvalues / total_variance\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(f\"\\nTotal variance (sum of eigenvalues): {total_variance:.4f}\")\n",
    "print(\"\\nExplained Variance by Each Principal Component:\")\n",
    "for i in range(min(10, len(explained_variance))):\n",
    "    print(f\"  PC{i+1}: {explained_variance[i]*100:.2f}% | Cumulative: {cumulative_variance[i]*100:.2f}%\")\n",
    "\n",
    "# Find thresholds for different variance levels\n",
    "threshold_80 = np.argmax(cumulative_variance >= 0.80) + 1\n",
    "threshold_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "threshold_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5C: COMPONENT SELECTION BASED ON VARIANCE THRESHOLDS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nStandard thresholds for dimensionality reduction:\")\n",
    "print(f\"  • 80% variance → Need {threshold_80} components (explains {cumulative_variance[threshold_80-1]*100:.2f}%)\")\n",
    "print(f\"  • 90% variance → Need {threshold_90} components (explains {cumulative_variance[threshold_90-1]*100:.2f}%)\")\n",
    "print(f\"  • 95% variance → Need {threshold_95} components (explains {cumulative_variance[threshold_95-1]*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n RECOMMENDATION: Use 95% threshold (standard practice in PCA)\")\n",
    "print(f\"   Selected components: {threshold_95}\")\n",
    "print(f\"   Dimensionality reduction: {len(sorted_eigenvalues)} → {threshold_95} features\")\n",
    "print(f\"   Reduction rate: {(1 - threshold_95/len(sorted_eigenvalues))*100:.1f}%\")\n",
    "\n",
    "# Visualizations\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5D: VISUALIZATIONS FOR COMPONENT SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Scree Plot (Elbow Method)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, min(16, len(explained_variance)+1)),\n",
    "         explained_variance[:15]*100, 'bo-', linewidth=2.5, markersize=10)\n",
    "plt.xlabel('Principal Component Number', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Explained Variance (%)', fontsize=12, fontweight='bold')\n",
    "plt.title('Scree Plot - Elbow Method\\n(Individual Variance per Component)',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.4)\n",
    "plt.xticks(range(1, 16))\n",
    "\n",
    "# Mark the \"elbow\" point\n",
    "elbow_point = 3\n",
    "plt.annotate('← Elbow Point\\n(Diminishing returns)',\n",
    "             xy=(elbow_point, explained_variance[elbow_point-1]*100),\n",
    "             xytext=(elbow_point+3, explained_variance[elbow_point-1]*100+10),\n",
    "             arrowprops=dict(arrowstyle='->', color='red', lw=2.5),\n",
    "             fontsize=11, color='red', fontweight='bold',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "# Plot 2: Bar Chart\n",
    "plt.subplot(1, 3, 2)\n",
    "colors = ['darkgreen' if i < threshold_95 else 'lightgray' for i in range(min(10, len(explained_variance)))]\n",
    "bars = plt.bar(range(1, min(11, len(explained_variance)+1)),\n",
    "               explained_variance[:10]*100, color=colors, edgecolor='black', linewidth=1.5)\n",
    "plt.xlabel('Principal Component Number', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Explained Variance (%)', fontsize=12, fontweight='bold')\n",
    "plt.title('Variance Contribution per Component\\n(Green = Selected for 95% threshold)',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Cumulative Variance (Threshold Lines)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(range(1, min(16, len(cumulative_variance)+1)),\n",
    "         cumulative_variance[:15]*100, 'ro-', linewidth=2.5, markersize=10)\n",
    "plt.axhline(y=80, color='green', linestyle='--', linewidth=2, label='80% Threshold')\n",
    "plt.axhline(y=90, color='orange', linestyle='--', linewidth=2, label='90% Threshold')\n",
    "plt.axhline(y=95, color='red', linestyle='--', linewidth=2.5, label='95% Threshold (Selected)')\n",
    "plt.axvline(x=threshold_95, color='red', linestyle=':', linewidth=2, alpha=0.7)\n",
    "plt.xlabel('Number of Components', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Cumulative Explained Variance (%)', fontsize=12, fontweight='bold')\n",
    "plt.title('Cumulative Variance Explained\\n(Threshold Selection)',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10, framealpha=0.9)\n",
    "plt.grid(True, alpha=0.4)\n",
    "plt.xticks(range(1, 16))\n",
    "plt.ylim([0, 105])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"JUSTIFICATION FOR COMPONENT SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\"\"\n",
    "WHY I SELECTED {threshold_95} PRINCIPAL COMPONENTS:\n",
    "\n",
    "1. EIGENVALUE ORDERING (Critical!):\n",
    "   • Sorted eigenvalues in DESCENDING order (largest → smallest)\n",
    "   • This ensures PC1 captures maximum variance, PC2 second-most, etc.\n",
    "   • Without sorting, PCA would be meaningless!\n",
    "\n",
    "2. SCREE PLOT ANALYSIS (Elbow Method):\n",
    "   • The scree plot shows a sharp \"elbow\" around PC{elbow_point}\n",
    "   • After PC{elbow_point}, additional components provide diminishing returns\n",
    "   • This suggests {elbow_point}-{elbow_point+2} components may be sufficient\n",
    "\n",
    "3. CUMULATIVE VARIANCE THRESHOLD (Standard Practice):\n",
    "   • 95% is the gold standard in PCA for retaining information\n",
    "   • Our {threshold_95} components explain {cumulative_variance[threshold_95-1]*100:.2f}% of variance\n",
    "   • We lose only {100-cumulative_variance[threshold_95-1]*100:.2f}% of information (acceptable!)\n",
    "\n",
    "4. DIMENSIONALITY REDUCTION ACHIEVED:\n",
    "   • Original: {len(sorted_eigenvalues)} features\n",
    "   • Reduced: {threshold_95} features\n",
    "   • Reduction: {(1-threshold_95/len(sorted_eigenvalues))*100:.1f}% fewer dimensions\n",
    "   • Benefit: Faster computation, less overfitting, easier visualization\n",
    "\n",
    "CONCLUSION: {threshold_95} components provide optimal balance between\n",
    "information retention ({cumulative_variance[threshold_95-1]*100:.2f}%) and dimensionality reduction.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxbmuO27r_Kg"
   },
   "source": [
    "### Step 6: Project Data onto Principal Components\n",
    "Now that we’ve selected the number of components, we will project the original data onto the chosen principal components.\n",
    "Fill in the code to perform the projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubjIMAtWr_Kg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5c73449d-a757-4558-ade4-3e3578241fe3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 6: Project Data onto Principal Components\n",
    "# Select number of components that explain at least 95% of variance\n",
    "num_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"Number of components selected (95% variance): {num_components}\")\n",
    "print(f\"Total variance explained: {cumulative_variance[num_components-1]*100:.2f}%\")\n",
    "\n",
    "# Project data onto the principal components\n",
    "selected_eigenvectors = sorted_eigenvectors[:, :num_components]\n",
    "reduced_data = np.dot(standardized_data, selected_eigenvectors)\n",
    "\n",
    "print(f\"\\nOriginal Data Shape: {standardized_data.shape}\")\n",
    "print(f\"Reduced Data Shape: {reduced_data.shape}\")\n",
    "print(f\"Dimensionality reduction: {standardized_data.shape[1]} → {reduced_data.shape[1]}\")\n",
    "print(f\"Reduction rate: {(1 - num_components/len(sorted_eigenvalues)) * 100:.1f}%\")\n",
    "print(reduced_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVq-b6vtr_Kg"
   },
   "source": [
    "### Step 7: Output the Reduced Data\n",
    "Finally, display the reduced data obtained by projecting the original dataset onto the selected principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D0uzRyPr_Kg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "610d5df9-8aac-4275-cdc7-68040a235ba6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 7: Output the Reduced Data\n",
    "print(f'Reduced Data Shape: {reduced_data.shape}')\n",
    "print(f'Original number of features: {standardized_data.shape[1]}')\n",
    "print(f'Reduced number of features: {reduced_data.shape[1]}')\n",
    "print(f'Variance retained: {cumulative_variance[num_components-1]*100:.2f}%')\n",
    "print(f'\\nFirst 5 rows of reduced data:')\n",
    "reduced_data[:5]  # Display the first few rows of reduced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1f8ROm5r_Kg"
   },
   "source": [
    "### Step 8: Visualize Before and After PCA\n",
    "Now, let's plot the original data and the data after PCA to compare the reduction in dimensions visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwuppWV-r_Kg",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "outputId": "8ec54e68-af98-474d-ac6d-27c1f222334a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 8: Visualize Before and After PCA\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot original data (first two features)\n",
    "axes[0].scatter(standardized_data.iloc[:, 0], standardized_data.iloc[:, 1],\n",
    "                alpha=0.6, c='blue', edgecolors='k', s=50)\n",
    "axes[0].set_xlabel(f'{numeric_df.columns[0]} (Standardized)', fontsize=12)\n",
    "axes[0].set_ylabel(f'{numeric_df.columns[1]} (Standardized)', fontsize=12)\n",
    "axes[0].set_title('Original Feature Space (Before PCA)\\nFirst Two Features', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Plot reduced data after PCA (first two principal components)\n",
    "axes[1].scatter(reduced_data[:, 0], reduced_data[:, 1],\n",
    "                alpha=0.6, c='red', edgecolors='k', s=50)\n",
    "axes[1].set_xlabel(f'PC1 ({explained_variance[0]*100:.1f}% variance)', fontsize=12)\n",
    "axes[1].set_ylabel(f'PC2 ({explained_variance[1]*100:.1f}% variance)', fontsize=12)\n",
    "axes[1].set_title('Principal Component Space (After PCA)\\nPC1 vs PC2', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization Explanation:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"BEFORE PCA (Left Plot):\")\n",
    "print(f\"  - Shows original features: {numeric_df.columns[0]} vs {numeric_df.columns[1]}\")\n",
    "print(\"  - Data is centered at origin (mean = 0) due to standardization\")\n",
    "print(\"  - Features may be correlated\")\n",
    "print(\"\\nAFTER PCA (Right Plot):\")\n",
    "print(\"  - Shows principal components PC1 vs PC2\")\n",
    "print(f\"  - PC1 captures {explained_variance[0]*100:.1f}% of total variance\")\n",
    "print(f\"  - PC2 captures {explained_variance[1]*100:.1f}% of total variance\")\n",
    "print(f\"  - Together: {(explained_variance[0]+explained_variance[1])*100:.1f}% of variance\")\n",
    "print(\"  - PCs are uncorrelated (orthogonal)\")\n",
    "print(\"  - Data structure preserved but rotated to maximize variance\")\n",
    "print(\"  - Same number of data points, clusters remain visible\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}